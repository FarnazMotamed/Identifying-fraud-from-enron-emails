# Identifying-fraud-from-enron-emails

the html link is <https://rawgit.com/FarnazMotamed/Identifying-fraud-from-enron-emails/master/%2Bidentifying-fraud-from-enron-email%2B-2.html>


Enron Submission Free-Response Questions

1.	Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]

The Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.[4][5]
The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.
The Enron data set has total number of 146 data points that each are representing a person. In this data set 1 of them is not a real person and one of them is not an important person for this assignment( as in , it is not a person of interest).
18 of these data points are labeled as POI( person of interest) and 128 are labeled as non-POI
there are 21 features in total that are assigned ( in one form or another) to people or data points. 14 financial features, 6 emails feature and 1 labeled.
Financial features:
['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', director_fees'] # (Units = USD) 
Email features:
['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'poi', 'shared_receipt_with_poi'] # units = number of emails messages; except ‘email_address’, which is a text string
POI label: [‘poi’] (boolean, represented as integers)
I did try to plot the outliers and find out which ones are the most relevant to be removed, while TOTAL is an obvious choice the rest was a little mind boggling. After googling and reading a lot of different solutions I ,in particular, found this equation very useful(Tukey Method):
outliers = df.quantile(.5) + 1.5 * (df.quantile(.75)-df.quantile(.25))
pd.DataFrame((df[1:] > outliers[1:]).sum(axis = 1), columns = ['# of outliers']).\ sort_values('# of outliers',  ascending = [0]).head(7)
in which the output is the following table:

#of outliers
TOTAL
11
FREVERT MARK A
10
WHALLEY LAWRENCE G
9
LAVORATO JOHN J
9
LAY KENNETH L
8
BELDEN TIMOTHY N
8
SKILLING JEFFREY K
7

TOTAL is the most obvious outlier which will be removed since it Is only the total value of financial payments from the FindLaw data. As it's doesn't make any sense for our solution.
 As an example, Lay and Skilling also have extreme values however, these are legitimate values of actual POIs, so they were not eliminated. All other names are a name of a person, hence I will keep them in oy dataset. I only removed those words that are not representing any person. I searched manually through the pdf file and found 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E' that are not representing any person, therefore at the end I removed three outliers.
leaving 143 data points.


2.	What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]



I selected the features that were used in the final POI identification algorithm using the feature importances from the DecisionTreeClassifier, shown below.
['bonus', 0.32605459057071978]
['expenses', 0.17127277342331099]
['restricted_stock', 0.12053540329402383]
['total_payments', 0.087214372928658621]
['from_this_person_to_poi', 0.064491475602586834]
['total_stock_value', 0.058682909257622026]
['from_poi_to_this_person', 0.055555555555555559]
['salary', 0.054421768707483005]
['exercised_stock_options', 0.031746031746031744]
['other', 0.030025118914007647]
However at the end I realized that this is not a good way of doing it, because I had to apply feature importance to a classifier in which my dataset has been fitted. And in that early stage of my coding, it is not a sensible way of doing so. I ended up making another decision, keeping all of the available features including emails and financial features. 
two new features was created as ‘fraction-from-poi’ and 'fraction_to_poi' = poi_messages / all_messages


3.	What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]


I did select Logistic Regression as final algorithm after testing 4 different machine learning algorithms. I tested Naïve-bayes, linear regression, kmeans, svc (support vector machine) and random forest. 
After evaluation their performances, it appears that logistic regression is behaving the best in this data set.
For the evaluation, I evaluated the precision and recall for each algorithm and then compared them in order to pick the most accurate one. 
For LogisticRegression: 
precision: 0.26287
recall:    0.38300
For KMeans:
precision: 0.0582076153737
recall:    0.0776678571429
For svc:
precision: 0.0
recall:    0.0
For RandomForest:
precision: 0.24269047619
recall:    0.114279978355



4.	What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]



Parameters tuning refers to the adjustment of the algorithm when training, in order to improve the fit on the test set. Parameter can influence the outcome of the learning process, the more tuned the parameters, the more biased the algorithm will be to the training data & test harness. Over turning the parameters could be a dangerous strategy because it can also lead to more fragile models and overfitting the test data-set but don't perform well in practice in a general data-set.
With every algorithms, I tried to tune as much as I could with only marginal success and insignificant improvement but came up with significant success with the Logistic Regression. Reading and searching through the documentation online, I came up with these following elements and factors:
Logistic regression: C (inverse regularization), class weight (weights associated with classes), max iteration (maximum number of iterations taken for the solvers to converge), random_state (the seed of the pseudo random number generator to use when shuffling the data), solver (using 'liblinear' since we have very small dataset).
C=1e-08, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, 
max_iter=100, multi_class='ovr', penalty='l2', random_state=42, solver='liblinear', tol=0.001, verbose=0))


5.	What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]


I defined a new function in order to evaluate and validate my analysis: def evaluate_clf(clf, features, labels, num_iters=1000, test_size=0.33).
I chose to iterate through 1000 times and to take 33% of the dataset to test the algorithm on them.
Validation in general is a set of techniques that can make sure that our model is able to generalize to any data set and is not bias or over fit to our current training data set. A single split into a training and test set would not give a better estimate of error accuracy. Therefore, we need to randomly split the data into multiple trials while keeping the fraction of POIs in each trials relatively constant.


6.	Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]

As two main evaluation factors for this assignment, I used precision and recall. The best performance belongs to logistic regression (precision: 0.382 & recall: 0.415) which is also the final model of choice, as logistic regression is also widely used in text classification, we can extend this model for email classification if needed. Precision refer to the ratio of true positive (predicted as POI) to the records that are in fact POI while recall described ratio of true positives to people flagged as POI. In other words, with a precision score of 0.386, it tells us if this model predicts 100 POIs, there would be 38 people are actually POIs and the rest 62 are innocent. With recall score of 0.4252, this model finds 42% of all real POIs in prediction. This model is effective for finding those who were involved with the fraud without missing anyone, but with 42% probability of wrongness.
On the other hand, if the recall is .415, our model is able to find 42% of all real POIs in prediction. Simply due to the nature of the dataset, accuracy is not a good measurement as even if non-POI are all flagged and labeled.





References:
1) http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/
2) http://www.infosys.tuwien.ac.at/staff/dschall/email/enron-employees.txt
3) https://en.wikipedia.org/wiki/Precision_and_recall
4) https://en.wikipedia.org/wiki/Enron_Corpus
5) https://en.wikipedia.org/wiki/Enron_scandal

